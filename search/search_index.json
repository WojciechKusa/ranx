{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\ud83d\udd25 News \ud83d\udccc [October 10, 2022] I released a new sharing platform for pre-computed runs called ranxhub , click here to learn more! [November 2, 2022] ranx 0.3.3 is out! This release adds support for changing Qrels relevance level, i.e, the minimum relevance judgement score to consider a document to be relevant. You can now define metric-wise relevance levels by appending -l<num> to metric names (e.g., evaluate(qrels, run, [\"map@100-l2\", \"ndcg-l3]) ), or setting the Qrels relevance level qrels-wise as qrels.set_relevance_level(2) . [October 10, 2022] ranx 0.3 is out! This release adds integration with ranxhub , a new sharing platform for pre-computed runs. Click here for a quick example. Click here to learn how to share your own runs with the community and lead by example! \u26a1\ufe0f Introduction ranx is a library of fast ranking evaluation metrics implemented in Python , leveraging Numba for high-speed vector operations and automatic parallelization . It offers a user-friendly interface to evaluate and compare Information Retrieval and Recommender Systems . ranx allows you to perform statistical tests and export LaTeX tables for your scientific publications. Moreover, ranx provides several fusion algorithms and normalization strategies , and an automatic fusion optimization functionality. ranx was featured in ECIR 2022 and CIKM 2022 . If you use ranx to evaluate results or conducting experiments involving fusion for your scientific publication, please consider it: evaluation bibtex , fusion bibtex . For a quick overview, follow the Usage section. For a in-depth overview, follow the Examples section. \u2728 Features Metrics Hits Hit Rate Precision Recall F1 r-Precision Bpref Rank-biased Precision (RBP) Mean Reciprocal Rank (MRR) Mean Average Precision (MAP) Normalized Discounted Cumulative Gain (NDCG) The metrics have been tested against TREC Eval for correctness. Statistical Tests Fisher's Randomization Test Paired Student's t-Test (default) Tukey's HSD Test Please, refer to Smucker et al. , Carterette , and Fuhr for additional information on statistical tests for Information Retrieval. Off-the-shelf Qrels You can load qrels from ir-datasets as simply as: qrels = Qrels . from_ir_datasets ( \"msmarco-document/dev\" ) A full list of the available qrels is provided here . Off-the-shelf Runs You can load runs from ranxhub as simply as: run = Run . from_ranxhub ( \"run-id\" ) A full list of the available runs is provided here . Fusion Algorithms Name Name Name Name Name CombMIN CombMNZ RRF MAPFuse BordaFuse CombMED CombGMNZ RBC PosFuse Weighted BordaFuse CombANZ ISR WMNZ ProbFuse Condorcet CombMAX Log_ISR Mixed SegFuse Weighted Condorcet CombSUM LogN_ISR BayesFuse SlideFuse Weighted Sum Please, refer to the documentation for further details. Normalization Strategies Min-Max Norm Max Norm Sum Norm ZMUV Norm Rank Norm Borda Norm Please, refer to the documentation for further details. \ud83d\udd0c Installation pip install ranx \ud83d\udca1 Usage Create Qrels and Run from ranx import Qrels , Run qrels_dict = { \"q_1\" : { \"d_12\" : 5 , \"d_25\" : 3 }, \"q_2\" : { \"d_11\" : 6 , \"d_22\" : 1 } } run_dict = { \"q_1\" : { \"d_12\" : 0.9 , \"d_23\" : 0.8 , \"d_25\" : 0.7 , \"d_36\" : 0.6 , \"d_32\" : 0.5 , \"d_35\" : 0.4 }, \"q_2\" : { \"d_12\" : 0.9 , \"d_11\" : 0.8 , \"d_25\" : 0.7 , \"d_36\" : 0.6 , \"d_22\" : 0.5 , \"d_35\" : 0.4 } } qrels = Qrels ( qrels_dict ) run = Run ( run_dict ) Evaluate from ranx import evaluate # Compute score for a single metric evaluate ( qrels , run , \"ndcg@5\" ) >>> 0.7861 # Compute scores for multiple metrics at once evaluate ( qrels , run , [ \"map@5\" , \"mrr\" ]) >>> { \"map@5\" : 0.6416 , \"mrr\" : 0.75 } Compare from ranx import compare # Compare different runs and perform Two-sided Paired Student's t-Test report = compare ( qrels = qrels , runs = [ run_1 , run_2 , run_3 , run_4 , run_5 ], metrics = [ \"map@100\" , \"mrr@100\" , \"ndcg@10\" ], max_p = 0.01 # P-value threshold ) Output: print ( report ) # Model MAP@100 MRR@100 NDCG@10 --- ------- -------- -------- --------- a model_1 0.320\u1d47 0.320\u1d47 0.368\u1d47\u1d9c b model_2 0.233 0.234 0.239 c model_3 0.308\u1d47 0.309\u1d47 0.330\u1d47 d model_4 0.366\u1d43\u1d47\u1d9c 0.367\u1d43\u1d47\u1d9c 0.408\u1d43\u1d47\u1d9c e model_5 0.405\u1d43\u1d47\u1d9c\u1d48 0.406\u1d43\u1d47\u1d9c\u1d48 0.451\u1d43\u1d47\u1d9c\u1d48 Fusion from ranx import fuse , optimize_fusion best_params = optimize_fusion ( qrels = train_qrels , runs = [ train_run_1 , train_run_2 , train_run_3 ], norm = \"min-max\" , # The norm. to apply before fusion method = \"wsum\" , # The fusion algorithm to use (Weighted Sum) metric = \"ndcg@100\" , # The metric to maximize ) combined_test_run = fuse ( runs = [ test_run_1 , test_run_2 , test_run_3 ], norm = \"min-max\" , method = \"wsum\" , params = best_params , ) \ud83d\udcd6 Examples Name Link Overview Qrels and Run Evaluation Comparison and Report Fusion Share your runs with ranxhub \ud83d\udcda Documentation Browse the documentation for more details and examples. \ud83c\udf93 Citation If you use ranx to evaluate results for your scientific publication, please consider citing it: @inproceedings{bassani2022ranx, author = {Elias Bassani}, title = {ranx: {A} Blazing-Fast Python Library for Ranking Evaluation and Comparison}, booktitle = {{ECIR} {(2)}}, series = {Lecture Notes in Computer Science}, volume = {13186}, pages = {259--264}, publisher = {Springer}, year = {2022} } \ud83c\udf81 Feature Requests Would you like to see other features implemented? Please, open a feature request . \ud83e\udd18 Want to contribute? Would you like to contribute? Please, drop me an e-mail . \ud83d\udcc4 License ranx is an open-sourced software licensed under the MIT license .","title":"Home"},{"location":"#news","text":"\ud83d\udccc [October 10, 2022] I released a new sharing platform for pre-computed runs called ranxhub , click here to learn more! [November 2, 2022] ranx 0.3.3 is out! This release adds support for changing Qrels relevance level, i.e, the minimum relevance judgement score to consider a document to be relevant. You can now define metric-wise relevance levels by appending -l<num> to metric names (e.g., evaluate(qrels, run, [\"map@100-l2\", \"ndcg-l3]) ), or setting the Qrels relevance level qrels-wise as qrels.set_relevance_level(2) . [October 10, 2022] ranx 0.3 is out! This release adds integration with ranxhub , a new sharing platform for pre-computed runs. Click here for a quick example. Click here to learn how to share your own runs with the community and lead by example!","title":"\ud83d\udd25 News"},{"location":"#introduction","text":"ranx is a library of fast ranking evaluation metrics implemented in Python , leveraging Numba for high-speed vector operations and automatic parallelization . It offers a user-friendly interface to evaluate and compare Information Retrieval and Recommender Systems . ranx allows you to perform statistical tests and export LaTeX tables for your scientific publications. Moreover, ranx provides several fusion algorithms and normalization strategies , and an automatic fusion optimization functionality. ranx was featured in ECIR 2022 and CIKM 2022 . If you use ranx to evaluate results or conducting experiments involving fusion for your scientific publication, please consider it: evaluation bibtex , fusion bibtex . For a quick overview, follow the Usage section. For a in-depth overview, follow the Examples section.","title":"\u26a1\ufe0f Introduction"},{"location":"#features","text":"","title":"\u2728 Features"},{"location":"#metrics","text":"Hits Hit Rate Precision Recall F1 r-Precision Bpref Rank-biased Precision (RBP) Mean Reciprocal Rank (MRR) Mean Average Precision (MAP) Normalized Discounted Cumulative Gain (NDCG) The metrics have been tested against TREC Eval for correctness.","title":"Metrics"},{"location":"#statistical-tests","text":"Fisher's Randomization Test Paired Student's t-Test (default) Tukey's HSD Test Please, refer to Smucker et al. , Carterette , and Fuhr for additional information on statistical tests for Information Retrieval.","title":"Statistical Tests"},{"location":"#off-the-shelf-qrels","text":"You can load qrels from ir-datasets as simply as: qrels = Qrels . from_ir_datasets ( \"msmarco-document/dev\" ) A full list of the available qrels is provided here .","title":"Off-the-shelf Qrels"},{"location":"#off-the-shelf-runs","text":"You can load runs from ranxhub as simply as: run = Run . from_ranxhub ( \"run-id\" ) A full list of the available runs is provided here .","title":"Off-the-shelf Runs"},{"location":"#fusion-algorithms","text":"Name Name Name Name Name CombMIN CombMNZ RRF MAPFuse BordaFuse CombMED CombGMNZ RBC PosFuse Weighted BordaFuse CombANZ ISR WMNZ ProbFuse Condorcet CombMAX Log_ISR Mixed SegFuse Weighted Condorcet CombSUM LogN_ISR BayesFuse SlideFuse Weighted Sum Please, refer to the documentation for further details.","title":"Fusion Algorithms"},{"location":"#normalization-strategies","text":"Min-Max Norm Max Norm Sum Norm ZMUV Norm Rank Norm Borda Norm Please, refer to the documentation for further details.","title":"Normalization Strategies"},{"location":"#installation","text":"pip install ranx","title":"\ud83d\udd0c Installation"},{"location":"#usage","text":"","title":"\ud83d\udca1 Usage"},{"location":"#create-qrels-and-run","text":"from ranx import Qrels , Run qrels_dict = { \"q_1\" : { \"d_12\" : 5 , \"d_25\" : 3 }, \"q_2\" : { \"d_11\" : 6 , \"d_22\" : 1 } } run_dict = { \"q_1\" : { \"d_12\" : 0.9 , \"d_23\" : 0.8 , \"d_25\" : 0.7 , \"d_36\" : 0.6 , \"d_32\" : 0.5 , \"d_35\" : 0.4 }, \"q_2\" : { \"d_12\" : 0.9 , \"d_11\" : 0.8 , \"d_25\" : 0.7 , \"d_36\" : 0.6 , \"d_22\" : 0.5 , \"d_35\" : 0.4 } } qrels = Qrels ( qrels_dict ) run = Run ( run_dict )","title":"Create Qrels and Run"},{"location":"#evaluate","text":"from ranx import evaluate # Compute score for a single metric evaluate ( qrels , run , \"ndcg@5\" ) >>> 0.7861 # Compute scores for multiple metrics at once evaluate ( qrels , run , [ \"map@5\" , \"mrr\" ]) >>> { \"map@5\" : 0.6416 , \"mrr\" : 0.75 }","title":"Evaluate"},{"location":"#compare","text":"from ranx import compare # Compare different runs and perform Two-sided Paired Student's t-Test report = compare ( qrels = qrels , runs = [ run_1 , run_2 , run_3 , run_4 , run_5 ], metrics = [ \"map@100\" , \"mrr@100\" , \"ndcg@10\" ], max_p = 0.01 # P-value threshold ) Output: print ( report ) # Model MAP@100 MRR@100 NDCG@10 --- ------- -------- -------- --------- a model_1 0.320\u1d47 0.320\u1d47 0.368\u1d47\u1d9c b model_2 0.233 0.234 0.239 c model_3 0.308\u1d47 0.309\u1d47 0.330\u1d47 d model_4 0.366\u1d43\u1d47\u1d9c 0.367\u1d43\u1d47\u1d9c 0.408\u1d43\u1d47\u1d9c e model_5 0.405\u1d43\u1d47\u1d9c\u1d48 0.406\u1d43\u1d47\u1d9c\u1d48 0.451\u1d43\u1d47\u1d9c\u1d48","title":"Compare"},{"location":"#fusion","text":"from ranx import fuse , optimize_fusion best_params = optimize_fusion ( qrels = train_qrels , runs = [ train_run_1 , train_run_2 , train_run_3 ], norm = \"min-max\" , # The norm. to apply before fusion method = \"wsum\" , # The fusion algorithm to use (Weighted Sum) metric = \"ndcg@100\" , # The metric to maximize ) combined_test_run = fuse ( runs = [ test_run_1 , test_run_2 , test_run_3 ], norm = \"min-max\" , method = \"wsum\" , params = best_params , )","title":"Fusion"},{"location":"#examples","text":"Name Link Overview Qrels and Run Evaluation Comparison and Report Fusion Share your runs with ranxhub","title":"\ud83d\udcd6 Examples"},{"location":"#documentation","text":"Browse the documentation for more details and examples.","title":"\ud83d\udcda Documentation"},{"location":"#citation","text":"If you use ranx to evaluate results for your scientific publication, please consider citing it: @inproceedings{bassani2022ranx, author = {Elias Bassani}, title = {ranx: {A} Blazing-Fast Python Library for Ranking Evaluation and Comparison}, booktitle = {{ECIR} {(2)}}, series = {Lecture Notes in Computer Science}, volume = {13186}, pages = {259--264}, publisher = {Springer}, year = {2022} }","title":"\ud83c\udf93 Citation"},{"location":"#feature-requests","text":"Would you like to see other features implemented? Please, open a feature request .","title":"\ud83c\udf81 Feature Requests"},{"location":"#want-to-contribute","text":"Would you like to contribute? Please, drop me an e-mail .","title":"\ud83e\udd18 Want to contribute?"},{"location":"#license","text":"ranx is an open-sourced software licensed under the MIT license .","title":"\ud83d\udcc4 License"},{"location":"compare/","text":"compare ( qrels , runs , metrics , stat_test = 'student' , n_permutations = 1000 , max_p = 0.01 , random_seed = 42 , threads = 0 , rounding_digits = 3 , show_percentages = False ) Evaluate multiple runs and compute statistical tests. Usage example: from ranx import compare # Compare different runs and perform statistical tests report = compare ( qrels = qrels , runs = [ run_1 , run_2 , run_3 , run_4 , run_5 ], metrics = [ \"map@100\" , \"mrr@100\" , \"ndcg@10\" ], max_p = 0.01 # P-value threshold ) print ( report ) Output: # Model MAP@100 MRR@100 NDCG@10 --- ------- ---------- ---------- ---------- a model_1 0.3202\u1d47 0.3207\u1d47 0.3684\u1d47\u1d9c b model_2 0.2332 0.2339 0.239 c model_3 0.3082\u1d47 0.3089\u1d47 0.3295\u1d47 d model_4 0.3664\u1d43\u1d47\u1d9c 0.3668\u1d43\u1d47\u1d9c 0.4078\u1d43\u1d47\u1d9c e model_5 0.4053\u1d43\u1d47\u1d9c\u1d48 0.4061\u1d43\u1d47\u1d9c\u1d48 0.4512\u1d43\u1d47\u1d9c\u1d48 Parameters: Name Type Description Default qrels Qrels Qrels. required runs List [ Run ] List of runs. required metrics Union [ List [ str ], str ] Metric or list of metrics. required n_permutations int Number of permutation to perform during statistical testing (Fisher's Randomization Test is used by default). Defaults to 1000. 1000 max_p float Maximum p-value to consider an increment as statistically significant. Defaults to 0.01. 0.01 stat_test str Statistical test to perform. Use \"fisher\" for Fisher's Randomization Test , \"student\" for Two-sided Paired Student's t-Test , or \"Tukey\" for Tukey's HSD test . Defaults to \"fisher\". 'student' random_seed int Random seed to use for generating the permutations. Defaults to 42. 42 threads int Number of threads to use, zero means all the available threads. Defaults to 0. 0 rounding_digits int Number of digits to round to and to show in the Report. Defaults to 3. 3 show_percentages bool Whether to show percentages instead of floats in the Report. Defaults to False. False Returns: Name Type Description Report Report See report.","title":"Compare"},{"location":"compare/#ranx.meta.compare.compare","text":"Evaluate multiple runs and compute statistical tests. Usage example: from ranx import compare # Compare different runs and perform statistical tests report = compare ( qrels = qrels , runs = [ run_1 , run_2 , run_3 , run_4 , run_5 ], metrics = [ \"map@100\" , \"mrr@100\" , \"ndcg@10\" ], max_p = 0.01 # P-value threshold ) print ( report ) Output: # Model MAP@100 MRR@100 NDCG@10 --- ------- ---------- ---------- ---------- a model_1 0.3202\u1d47 0.3207\u1d47 0.3684\u1d47\u1d9c b model_2 0.2332 0.2339 0.239 c model_3 0.3082\u1d47 0.3089\u1d47 0.3295\u1d47 d model_4 0.3664\u1d43\u1d47\u1d9c 0.3668\u1d43\u1d47\u1d9c 0.4078\u1d43\u1d47\u1d9c e model_5 0.4053\u1d43\u1d47\u1d9c\u1d48 0.4061\u1d43\u1d47\u1d9c\u1d48 0.4512\u1d43\u1d47\u1d9c\u1d48 Parameters: Name Type Description Default qrels Qrels Qrels. required runs List [ Run ] List of runs. required metrics Union [ List [ str ], str ] Metric or list of metrics. required n_permutations int Number of permutation to perform during statistical testing (Fisher's Randomization Test is used by default). Defaults to 1000. 1000 max_p float Maximum p-value to consider an increment as statistically significant. Defaults to 0.01. 0.01 stat_test str Statistical test to perform. Use \"fisher\" for Fisher's Randomization Test , \"student\" for Two-sided Paired Student's t-Test , or \"Tukey\" for Tukey's HSD test . Defaults to \"fisher\". 'student' random_seed int Random seed to use for generating the permutations. Defaults to 42. 42 threads int Number of threads to use, zero means all the available threads. Defaults to 0. 0 rounding_digits int Number of digits to round to and to show in the Report. Defaults to 3. 3 show_percentages bool Whether to show percentages instead of floats in the Report. Defaults to False. False Returns: Name Type Description Report Report See report.","title":"compare()"},{"location":"evaluate/","text":"evaluate ( qrels , run , metrics , return_mean = True , threads = 0 , save_results_in_run = True ) Compute the performance scores for the provided qrels and run for all the specified metrics. Usage examples: from ranx import evaluate # Compute score for a single metric evaluate ( qrels , run , \"ndcg@5\" ) >>> 0.7861 # Compute scores for multiple metrics at once evaluate ( qrels , run , [ \"map@5\" , \"mrr\" ]) >>> { \"map@5\" : 0.6416 , \"mrr\" : 0.75 } # Computed metric scores are saved in the Run object run . mean_scores >>> { \"ndcg@5\" : 0.7861 , \"map@5\" : 0.6416 , \"mrr\" : 0.75 } # Access scores for each query dict ( run . scores ) >>> { \"ndcg@5\" : { \"q_1\" : 0.9430 , \"q_2\" : 0.6292 }, \"map@5\" : { \"q_1\" : 0.8333 , \"q_2\" : 0.4500 }, \"mrr\" : { \"q_1\" : 1.0000 , \"q_2\" : 0.5000 }} Parameters: Name Type Description Default qrels Union [ Qrels , Dict [ str , Dict [ str , Number ]], nb . typed . typedlist . List , np . ndarray ] Qrels. required run Union [ Run , Dict [ str , Dict [ str , Number ]], nb . typed . typedlist . List , np . ndarray ] Run. required metrics Union [ List [ str ], str ] Metrics or list of metric to compute. required return_mean bool Wether to return the metric scores averaged over the query set or the scores for individual queries. Defaults to True. True threads int Number of threads to use, zero means all the available threads. Defaults to 0. 0 save_results_in_run bool Save metric scores for each query in the input run . Defaults to True. True Returns: Type Description Union [ Dict [ str , float ], float ] Union[Dict[str, float], float]: Results.","title":"Evaluate"},{"location":"evaluate/#ranx.meta.evaluate.evaluate","text":"Compute the performance scores for the provided qrels and run for all the specified metrics. Usage examples: from ranx import evaluate # Compute score for a single metric evaluate ( qrels , run , \"ndcg@5\" ) >>> 0.7861 # Compute scores for multiple metrics at once evaluate ( qrels , run , [ \"map@5\" , \"mrr\" ]) >>> { \"map@5\" : 0.6416 , \"mrr\" : 0.75 } # Computed metric scores are saved in the Run object run . mean_scores >>> { \"ndcg@5\" : 0.7861 , \"map@5\" : 0.6416 , \"mrr\" : 0.75 } # Access scores for each query dict ( run . scores ) >>> { \"ndcg@5\" : { \"q_1\" : 0.9430 , \"q_2\" : 0.6292 }, \"map@5\" : { \"q_1\" : 0.8333 , \"q_2\" : 0.4500 }, \"mrr\" : { \"q_1\" : 1.0000 , \"q_2\" : 0.5000 }} Parameters: Name Type Description Default qrels Union [ Qrels , Dict [ str , Dict [ str , Number ]], nb . typed . typedlist . List , np . ndarray ] Qrels. required run Union [ Run , Dict [ str , Dict [ str , Number ]], nb . typed . typedlist . List , np . ndarray ] Run. required metrics Union [ List [ str ], str ] Metrics or list of metric to compute. required return_mean bool Wether to return the metric scores averaged over the query set or the scores for individual queries. Defaults to True. True threads int Number of threads to use, zero means all the available threads. Defaults to 0. 0 save_results_in_run bool Save metric scores for each query in the input run . Defaults to True. True Returns: Type Description Union [ Dict [ str , float ], float ] Union[Dict[str, float], float]: Results.","title":"evaluate()"},{"location":"fusion/","text":"Fusion Fuse ranx provides several fusion algorithms , all of which can be accessed through a single function in the same fashion as evaluate . Usage example: from ranx import fuse combined_run = fuse ( runs = [ run_1 , run_2 ], # A list of Run instances to fuse norm = \"min-max\" , # The normalization strategy to apply before fusion method = \"max\" , # The fusion algorithm to use ) Optimize Fusion As many fusion algorithms require a training or optimization step, ranx provides a function to optimize all of those algorithms. For algorithms requiring hyper-parameter optimization, ranx automatically evaluates pre-defined configurations via grid search. In those cases, ranx shows a progress bar. Usage example: from ranx import fuse , optimize_fusion best_params = optimize_fusion ( qrels = train_qrels , runs = [ train_run_1 , train_run_2 , train_run_3 ], norm = \"min-max\" , method = \"wsum\" , metric = \"ndcg@100\" , # The metric to maximize during optimization ) combined_test_run = fuse ( runs = [ test_run_1 , test_run_2 , test_run_3 ], norm = \"min-max\" , method = \"wsum\" , params = best_params , ) Supported fusion algorithms ranx supports the following fusion algorithms: Algorithm Alias Optim. Algorithm Alias Optim. CombMIN min No CombMAX max No CombMED med No CombSUM sum No CombANZ anz No CombMNZ mnz No CombGMNZ gmnz Yes ISR isr No Log_ISR log_isr No LogN_ISR logn_isr Yes Reciprocal Rank Fusion (RRF) rrf Yes PosFuse posfuse Yes ProbFuse probfuse Yes SegFuse segfuse Yes SlideFuse slidefuse Yes MAPFuse mapfuse Yes BordaFuse bordafuse No Weighted BordaFuse w_bordafuse Yes Condorcet condorcet No Weighted Condorcet w_condorcet Yes BayesFuse bayesfuse Yes Mixed mixed Yes WMNZ wmnz Yes Wighted Sum wsum Yes Rank-Biased Centroids (RBC) rbc Yes BayesFuse Computes BayesFuse as proposed by Aslam et al. . BibTeX @inproceedings { DBLP:conf/sigir/AslamM01 , author = {Javed A. Aslam and Mark H. Montague} , editor = {W. Bruce Croft and David J. Harper and Donald H. Kraft and Justin Zobel} , title = {Models for Metasearch} , booktitle = {{SIGIR} 2001: Proceedings of the 24th Annual International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval, September 9-13, 2001, New Orleans, Louisiana, {USA}} , pages = {275--284} , publisher = {{ACM}} , year = {2001} , url = {https://doi.org/10.1145/383952.384007} , doi = {10.1145/383952.384007} , timestamp = {Tue, 06 Nov 2018 11:07:25 +0100} , biburl = {https://dblp.org/rec/conf/sigir/AslamM01.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} } BordaFuse Computes BordaFuse as proposed by Aslam et al. . BibTeX @inproceedings { DBLP:conf/sigir/AslamM01 , author = {Javed A. Aslam and Mark H. Montague} , editor = {W. Bruce Croft and David J. Harper and Donald H. Kraft and Justin Zobel} , title = {Models for Metasearch} , booktitle = {{SIGIR} 2001: Proceedings of the 24th Annual International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval, September 9-13, 2001, New Orleans, Louisiana, {USA}} , pages = {275--284} , publisher = {{ACM}} , year = {2001} , url = {https://doi.org/10.1145/383952.384007} , doi = {10.1145/383952.384007} , timestamp = {Tue, 06 Nov 2018 11:07:25 +0100} , biburl = {https://dblp.org/rec/conf/sigir/AslamM01.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} } CombANZ Computes CombANZ as proposed by Fox et al. . BibTeX @inproceedings { DBLP:conf/trec/FoxS93 , author = {Edward A. Fox and Joseph A. Shaw} , title = {Combination of Multiple Searches} , booktitle = {{TREC}} , series = {{NIST} Special Publication} , volume = {500-215} , pages = {243--252} , publisher = {National Institute of Standards and Technology {(NIST)}} , year = {1993} } CombGMNZ Computes CombGMNZ as proposed by Joon Ho Lee . BibTeX @inproceedings { DBLP:conf/sigir/Lee97 , author = {Joon Ho Lee} , title = {Analyses of Multiple Evidence Combination} , booktitle = {{SIGIR}} , pages = {267--276} , publisher = {{ACM}} , year = {1997} } Optimization Parameter Default Value min_gamma 0.01 max_gamma 1.0 step 0.01 CombMAX Computes CombMAX as proposed by Fox et al. . BibTeX @inproceedings { DBLP:conf/trec/FoxS93 , author = {Edward A. Fox and Joseph A. Shaw} , title = {Combination of Multiple Searches} , booktitle = {{TREC}} , series = {{NIST} Special Publication} , volume = {500-215} , pages = {243--252} , publisher = {National Institute of Standards and Technology {(NIST)}} , year = {1993} } CombMED Computes CombMED as proposed by Fox et al. . BibTeX @inproceedings { DBLP:conf/trec/FoxS93 , author = {Edward A. Fox and Joseph A. Shaw} , title = {Combination of Multiple Searches} , booktitle = {{TREC}} , series = {{NIST} Special Publication} , volume = {500-215} , pages = {243--252} , publisher = {National Institute of Standards and Technology {(NIST)}} , year = {1993} } CombMIN Computes CombMIN as proposed by Fox et al. . BibTeX @inproceedings { DBLP:conf/trec/FoxS93 , author = {Edward A. Fox and Joseph A. Shaw} , title = {Combination of Multiple Searches} , booktitle = {{TREC}} , series = {{NIST} Special Publication} , volume = {500-215} , pages = {243--252} , publisher = {National Institute of Standards and Technology {(NIST)}} , year = {1993} } CombMNZ Computes CombMNZ as proposed by Fox et al. . BibTeX @inproceedings { DBLP:conf/trec/FoxS93 , author = {Edward A. Fox and Joseph A. Shaw} , title = {Combination of Multiple Searches} , booktitle = {{TREC}} , series = {{NIST} Special Publication} , volume = {500-215} , pages = {243--252} , publisher = {National Institute of Standards and Technology {(NIST)}} , year = {1993} } CombSUM Computes CombSUM as proposed by Fox et al. . BibTeX @inproceedings { DBLP:conf/trec/FoxS93 , author = {Edward A. Fox and Joseph A. Shaw} , title = {Combination of Multiple Searches} , booktitle = {{TREC}} , series = {{NIST} Special Publication} , volume = {500-215} , pages = {243--252} , publisher = {National Institute of Standards and Technology {(NIST)}} , year = {1993} } Condorcet Computes Condorcet as proposed by Montague et al. . BibTeX @inproceedings { DBLP:conf/cikm/MontagueA02 , author = {Mark H. Montague and Javed A. Aslam} , title = {Condorcet fusion for improved retrieval} , booktitle = {Proceedings of the 2002 {ACM} {CIKM} International Conference on Information and Knowledge Management, McLean, VA, USA, November 4-9, 2002} , pages = {538--548} , publisher = {{ACM}} , year = {2002} , url = {https://doi.org/10.1145/584792.584881} , doi = {10.1145/584792.584881} , timestamp = {Tue, 06 Nov 2018 16:57:50 +0100} , biburl = {https://dblp.org/rec/conf/cikm/MontagueA02.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} } ISR Computes ISR as proposed by Mour\u00e3o et al. . BibTeX @article { DBLP:journals/cmig/MouraoMM15 , author = {Andr{\\'{e}} Mour{\\~{a}}o and Fl{\\'{a}}vio Martins and Jo{\\~{a}}o Magalh{\\~{a}}es} , title = {Multimodal medical information retrieval with unsupervised rank fusion} , journal = {Comput. Medical Imaging Graph.} , volume = {39} , pages = {35--45} , year = {2015} , url = {https://doi.org/10.1016/j.compmedimag.2014.05.006} , doi = {10.1016/j.compmedimag.2014.05.006} , timestamp = {Thu, 14 May 2020 10:17:16 +0200} , biburl = {https://dblp.org/rec/journals/cmig/MouraoMM15.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} } Log_ISR Computes Log_ISR as proposed by Mour\u00e3o et al. . BibTeX @article { DBLP:journals/cmig/MouraoMM15 , author = {Andr{\\'{e}} Mour{\\~{a}}o and Fl{\\'{a}}vio Martins and Jo{\\~{a}}o Magalh{\\~{a}}es} , title = {Multimodal medical information retrieval with unsupervised rank fusion} , journal = {Comput. Medical Imaging Graph.} , volume = {39} , pages = {35--45} , year = {2015} , url = {https://doi.org/10.1016/j.compmedimag.2014.05.006} , doi = {10.1016/j.compmedimag.2014.05.006} , timestamp = {Thu, 14 May 2020 10:17:16 +0200} , biburl = {https://dblp.org/rec/journals/cmig/MouraoMM15.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} } LogN_ISR Computes Log_ISR as proposed by Mour\u00e3o et al. . BibTeX @article { DBLP:journals/cmig/MouraoMM15 , author = {Andr{\\'{e}} Mour{\\~{a}}o and Fl{\\'{a}}vio Martins and Jo{\\~{a}}o Magalh{\\~{a}}es} , title = {Multimodal medical information retrieval with unsupervised rank fusion} , journal = {Comput. Medical Imaging Graph.} , volume = {39} , pages = {35--45} , year = {2015} , url = {https://doi.org/10.1016/j.compmedimag.2014.05.006} , doi = {10.1016/j.compmedimag.2014.05.006} , timestamp = {Thu, 14 May 2020 10:17:16 +0200} , biburl = {https://dblp.org/rec/journals/cmig/MouraoMM15.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} } Optimization Parameter Default Value min_sigma 0.01 max_sigma 1.0 step 0.01 MAPFuse Computes MAPFuse as proposed by Lillis et al. . BibTeX @inproceedings { DBLP:conf/sigir/LillisZTCLD10 , author = {David Lillis and Lusheng Zhang and Fergus Toolan and Rem W. Collier and David Leonard and John Dunnion} , editor = {Fabio Crestani and St{\\'{e}}phane Marchand{-}Maillet and Hsin{-}Hsi Chen and Efthimis N. Efthimiadis and Jacques Savoy} , title = {Estimating probabilities for effective data fusion} , booktitle = {Proceeding of the 33rd International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval, {SIGIR} 2010, Geneva, Switzerland, July 19-23, 2010} , pages = {347--354} , publisher = {{ACM}} , year = {2010} , url = {https://doi.org/10.1145/1835449.1835508} , doi = {10.1145/1835449.1835508} , timestamp = {Tue, 06 Nov 2018 11:07:25 +0100} , biburl = {https://dblp.org/rec/conf/sigir/LillisZTCLD10.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} } Mixed Computes Mixed as proposed by Wu et al. . BibTeX @inproceedings { DBLP:conf/cikm/WuC02 , author = {Shengli Wu and Fabio Crestani} , title = {Data fusion with estimated weights} , booktitle = {Proceedings of the 2002 {ACM} {CIKM} International Conference on Information and Knowledge Management, McLean, VA, USA, November 4-9, 2002} , pages = {648--651} , publisher = {{ACM}} , year = {2002} , url = {https://doi.org/10.1145/584792.584908} , doi = {10.1145/584792.584908} , timestamp = {Tue, 06 Nov 2018 16:57:40 +0100} , biburl = {https://dblp.org/rec/conf/cikm/WuC02.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} } Optimization Parameter Default Value step 0.1 PosFuse Computes PosFuse as proposed by Lillis et al. . BibTeX @inproceedings { DBLP:conf/sigir/LillisZTCLD10 , author = {David Lillis and Lusheng Zhang and Fergus Toolan and Rem W. Collier and David Leonard and John Dunnion} , editor = {Fabio Crestani and St{\\'{e}}phane Marchand{-}Maillet and Hsin{-}Hsi Chen and Efthimis N. Efthimiadis and Jacques Savoy} , title = {Estimating probabilities for effective data fusion} , booktitle = {Proceeding of the 33rd International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval, {SIGIR} 2010, Geneva, Switzerland, July 19-23, 2010} , pages = {347--354} , publisher = {{ACM}} , year = {2010} , url = {https://doi.org/10.1145/1835449.1835508} , doi = {10.1145/1835449.1835508} , timestamp = {Tue, 06 Nov 2018 11:07:25 +0100} , biburl = {https://dblp.org/rec/conf/sigir/LillisZTCLD10.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} } ProbFuse Computes ProbFuse as proposed by Lillis et al. . BibTeX @inproceedings { DBLP:conf/sigir/LillisTCD06 , author = {David Lillis and Fergus Toolan and Rem W. Collier and John Dunnion} , editor = {Efthimis N. Efthimiadis and Susan T. Dumais and David Hawking and Kalervo J{\\\"{a}}rvelin} , title = {ProbFuse: a probabilistic approach to data fusion} , booktitle = {{SIGIR} 2006: Proceedings of the 29th Annual International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval, Seattle, Washington, USA, August 6-11, 2006} , pages = {139--146} , publisher = {{ACM}} , year = {2006} , url = {https://doi.org/10.1145/1148170.1148197} , doi = {10.1145/1148170.1148197} , timestamp = {Wed, 14 Nov 2018 10:58:10 +0100} , biburl = {https://dblp.org/rec/conf/sigir/LillisTCD06.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} } Optimization Parameter Default Value min_n_segments 1 max_n_segments 100 Rank-Biased Centroids (RBC) Computes Rank-Biased Centroid (RBC) as proposed by Bailey et al. . BibTeX @inproceedings { DBLP:conf/sigir/BaileyMST17 , author = {Peter Bailey and Alistair Moffat and Falk Scholer and Paul Thomas} , editor = {Noriko Kando and Tetsuya Sakai and Hideo Joho and Hang Li and Arjen P. de Vries and Ryen W. White} , title = {Retrieval Consistency in the Presence of Query Variations} , booktitle = {Proceedings of the 40th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval, Shinjuku, Tokyo, Japan, August 7-11, 2017} , pages = {395--404} , publisher = {{ACM}} , year = {2017} , url = {https://doi.org/10.1145/3077136.3080839} , doi = {10.1145/3077136.3080839} , timestamp = {Wed, 25 Sep 2019 16:43:14 +0200} , biburl = {https://dblp.org/rec/conf/sigir/BaileyMST17.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} } Optimization Parameter Default Value min_phi 0.01 max_phi 1.0 step 0.01 Reciprocal Rank Fusion (RRF) Computes Reciprocal Rank Fusion as proposed by Cormack et al. . BibTeX @inproceedings { DBLP:conf/sigir/CormackCB09 , author = {Gordon V. Cormack and Charles L. A. Clarke and Stefan B{\\\"{u}}ttcher} , title = {Reciprocal rank fusion outperforms condorcet and individual rank learning methods} , booktitle = {{SIGIR}} , pages = {758--759} , publisher = {{ACM}} , year = {2009} } Optimization Parameter Default Value min_k 10 max_k 100 step 10 SegFuse Computes SegFuse as proposed by Shokouhi . BibTeX @inproceedings { DBLP:conf/ecir/Shokouhi07a , author = {Milad Shokouhi} , editor = {Giambattista Amati and Claudio Carpineto and Giovanni Romano} , title = {Segmentation of Search Engine Results for Effective Data-Fusion} , booktitle = {Advances in Information Retrieval, 29th European Conference on {IR} Research, {ECIR} 2007, Rome, Italy, April 2-5, 2007, Proceedings} , series = {Lecture Notes in Computer Science} , volume = {4425} , pages = {185--197} , publisher = {Springer} , year = {2007} , url = {https://doi.org/10.1007/978-3-540-71496-5\\_19} , doi = {10.1007/978-3-540-71496-5\\_19} , timestamp = {Tue, 14 May 2019 10:00:37 +0200} , biburl = {https://dblp.org/rec/conf/ecir/Shokouhi07a.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} } SlideFuse Computes SlideFuse as proposed by Lillis et al. . BibTeX @inproceedings { DBLP:conf/ecir/LillisTCD08 , author = {David Lillis and Fergus Toolan and Rem W. Collier and John Dunnion} , editor = {Craig Macdonald and Iadh Ounis and Vassilis Plachouras and Ian Ruthven and Ryen W. White} , title = {Extending Probabilistic Data Fusion Using Sliding Windows} , booktitle = {Advances in Information Retrieval , 30th European Conference on {IR} Research, {ECIR} 2008, Glasgow, UK, March 30-April 3, 2008. Proceedings} , series = {Lecture Notes in Computer Science} , volume = {4956} , pages = {358--369} , publisher = {Springer} , year = {2008} , url = {https://doi.org/10.1007/978-3-540-78646-7\\_33} , doi = {10.1007/978-3-540-78646-7\\_33} , timestamp = {Sun, 25 Oct 2020 22:33:08 +0100} , biburl = {https://dblp.org/rec/conf/ecir/LillisTCD08.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} } Optimization Parameter Default Value min_w 1 max_w 100 Weighted BordaFuse Computes Weighted BordaFuse as proposed by Aslam et al. . BibTeX @inproceedings { DBLP:conf/sigir/AslamM01 , author = {Javed A. Aslam and Mark H. Montague} , editor = {W. Bruce Croft and David J. Harper and Donald H. Kraft and Justin Zobel} , title = {Models for Metasearch} , booktitle = {{SIGIR} 2001: Proceedings of the 24th Annual International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval, September 9-13, 2001, New Orleans, Louisiana, {USA}} , pages = {275--284} , publisher = {{ACM}} , year = {2001} , url = {https://doi.org/10.1145/383952.384007} , doi = {10.1145/383952.384007} , timestamp = {Tue, 06 Nov 2018 11:07:25 +0100} , biburl = {https://dblp.org/rec/conf/sigir/AslamM01.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} } Optimization Parameter Default Value step 0.1 Weighted Condorcet Computes Weighted Condorcet as proposed by Montague et al. . BibTeX @inproceedings { DBLP:conf/cikm/MontagueA02 , author = {Mark H. Montague and Javed A. Aslam} , title = {Condorcet fusion for improved retrieval} , booktitle = {Proceedings of the 2002 {ACM} {CIKM} International Conference on Information and Knowledge Management, McLean, VA, USA, November 4-9, 2002} , pages = {538--548} , publisher = {{ACM}} , year = {2002} , url = {https://doi.org/10.1145/584792.584881} , doi = {10.1145/584792.584881} , timestamp = {Tue, 06 Nov 2018 16:57:50 +0100} , biburl = {https://dblp.org/rec/conf/cikm/MontagueA02.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} } Optimization Parameter Default Value step 0.1 WMNZ Computes Weighted MNZ as proposed by Wu et al. . BibTeX @inproceedings { DBLP:conf/cikm/WuC02 , author = {Shengli Wu and Fabio Crestani} , title = {Data fusion with estimated weights} , booktitle = {Proceedings of the 2002 {ACM} {CIKM} International Conference on Information and Knowledge Management, McLean, VA, USA, November 4-9, 2002} , pages = {648--651} , publisher = {{ACM}} , year = {2002} , url = {https://doi.org/10.1145/584792.584908} , doi = {10.1145/584792.584908} , timestamp = {Tue, 06 Nov 2018 16:57:40 +0100} , biburl = {https://dblp.org/rec/conf/cikm/WuC02.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} } Optimization Parameter Default Value step 0.1 Wighted Sum Computes a weighted sum of the scores given to documents by a list of Runs. Optimization Parameter Default Value step 0.1","title":"Fusion"},{"location":"fusion/#fusion","text":"","title":"Fusion"},{"location":"fusion/#fuse","text":"ranx provides several fusion algorithms , all of which can be accessed through a single function in the same fashion as evaluate . Usage example: from ranx import fuse combined_run = fuse ( runs = [ run_1 , run_2 ], # A list of Run instances to fuse norm = \"min-max\" , # The normalization strategy to apply before fusion method = \"max\" , # The fusion algorithm to use )","title":"Fuse"},{"location":"fusion/#optimize-fusion","text":"As many fusion algorithms require a training or optimization step, ranx provides a function to optimize all of those algorithms. For algorithms requiring hyper-parameter optimization, ranx automatically evaluates pre-defined configurations via grid search. In those cases, ranx shows a progress bar. Usage example: from ranx import fuse , optimize_fusion best_params = optimize_fusion ( qrels = train_qrels , runs = [ train_run_1 , train_run_2 , train_run_3 ], norm = \"min-max\" , method = \"wsum\" , metric = \"ndcg@100\" , # The metric to maximize during optimization ) combined_test_run = fuse ( runs = [ test_run_1 , test_run_2 , test_run_3 ], norm = \"min-max\" , method = \"wsum\" , params = best_params , )","title":"Optimize Fusion"},{"location":"fusion/#supported-fusion-algorithms","text":"ranx supports the following fusion algorithms: Algorithm Alias Optim. Algorithm Alias Optim. CombMIN min No CombMAX max No CombMED med No CombSUM sum No CombANZ anz No CombMNZ mnz No CombGMNZ gmnz Yes ISR isr No Log_ISR log_isr No LogN_ISR logn_isr Yes Reciprocal Rank Fusion (RRF) rrf Yes PosFuse posfuse Yes ProbFuse probfuse Yes SegFuse segfuse Yes SlideFuse slidefuse Yes MAPFuse mapfuse Yes BordaFuse bordafuse No Weighted BordaFuse w_bordafuse Yes Condorcet condorcet No Weighted Condorcet w_condorcet Yes BayesFuse bayesfuse Yes Mixed mixed Yes WMNZ wmnz Yes Wighted Sum wsum Yes Rank-Biased Centroids (RBC) rbc Yes","title":"Supported fusion algorithms"},{"location":"fusion/#bayesfuse","text":"Computes BayesFuse as proposed by Aslam et al. . BibTeX @inproceedings { DBLP:conf/sigir/AslamM01 , author = {Javed A. Aslam and Mark H. Montague} , editor = {W. Bruce Croft and David J. Harper and Donald H. Kraft and Justin Zobel} , title = {Models for Metasearch} , booktitle = {{SIGIR} 2001: Proceedings of the 24th Annual International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval, September 9-13, 2001, New Orleans, Louisiana, {USA}} , pages = {275--284} , publisher = {{ACM}} , year = {2001} , url = {https://doi.org/10.1145/383952.384007} , doi = {10.1145/383952.384007} , timestamp = {Tue, 06 Nov 2018 11:07:25 +0100} , biburl = {https://dblp.org/rec/conf/sigir/AslamM01.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"BayesFuse"},{"location":"fusion/#bordafuse","text":"Computes BordaFuse as proposed by Aslam et al. . BibTeX @inproceedings { DBLP:conf/sigir/AslamM01 , author = {Javed A. Aslam and Mark H. Montague} , editor = {W. Bruce Croft and David J. Harper and Donald H. Kraft and Justin Zobel} , title = {Models for Metasearch} , booktitle = {{SIGIR} 2001: Proceedings of the 24th Annual International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval, September 9-13, 2001, New Orleans, Louisiana, {USA}} , pages = {275--284} , publisher = {{ACM}} , year = {2001} , url = {https://doi.org/10.1145/383952.384007} , doi = {10.1145/383952.384007} , timestamp = {Tue, 06 Nov 2018 11:07:25 +0100} , biburl = {https://dblp.org/rec/conf/sigir/AslamM01.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"BordaFuse"},{"location":"fusion/#combanz","text":"Computes CombANZ as proposed by Fox et al. . BibTeX @inproceedings { DBLP:conf/trec/FoxS93 , author = {Edward A. Fox and Joseph A. Shaw} , title = {Combination of Multiple Searches} , booktitle = {{TREC}} , series = {{NIST} Special Publication} , volume = {500-215} , pages = {243--252} , publisher = {National Institute of Standards and Technology {(NIST)}} , year = {1993} }","title":"CombANZ"},{"location":"fusion/#combgmnz","text":"Computes CombGMNZ as proposed by Joon Ho Lee . BibTeX @inproceedings { DBLP:conf/sigir/Lee97 , author = {Joon Ho Lee} , title = {Analyses of Multiple Evidence Combination} , booktitle = {{SIGIR}} , pages = {267--276} , publisher = {{ACM}} , year = {1997} } Optimization Parameter Default Value min_gamma 0.01 max_gamma 1.0 step 0.01","title":"CombGMNZ"},{"location":"fusion/#combmax","text":"Computes CombMAX as proposed by Fox et al. . BibTeX @inproceedings { DBLP:conf/trec/FoxS93 , author = {Edward A. Fox and Joseph A. Shaw} , title = {Combination of Multiple Searches} , booktitle = {{TREC}} , series = {{NIST} Special Publication} , volume = {500-215} , pages = {243--252} , publisher = {National Institute of Standards and Technology {(NIST)}} , year = {1993} }","title":"CombMAX"},{"location":"fusion/#combmed","text":"Computes CombMED as proposed by Fox et al. . BibTeX @inproceedings { DBLP:conf/trec/FoxS93 , author = {Edward A. Fox and Joseph A. Shaw} , title = {Combination of Multiple Searches} , booktitle = {{TREC}} , series = {{NIST} Special Publication} , volume = {500-215} , pages = {243--252} , publisher = {National Institute of Standards and Technology {(NIST)}} , year = {1993} }","title":"CombMED"},{"location":"fusion/#combmin","text":"Computes CombMIN as proposed by Fox et al. . BibTeX @inproceedings { DBLP:conf/trec/FoxS93 , author = {Edward A. Fox and Joseph A. Shaw} , title = {Combination of Multiple Searches} , booktitle = {{TREC}} , series = {{NIST} Special Publication} , volume = {500-215} , pages = {243--252} , publisher = {National Institute of Standards and Technology {(NIST)}} , year = {1993} }","title":"CombMIN"},{"location":"fusion/#combmnz","text":"Computes CombMNZ as proposed by Fox et al. . BibTeX @inproceedings { DBLP:conf/trec/FoxS93 , author = {Edward A. Fox and Joseph A. Shaw} , title = {Combination of Multiple Searches} , booktitle = {{TREC}} , series = {{NIST} Special Publication} , volume = {500-215} , pages = {243--252} , publisher = {National Institute of Standards and Technology {(NIST)}} , year = {1993} }","title":"CombMNZ"},{"location":"fusion/#combsum","text":"Computes CombSUM as proposed by Fox et al. . BibTeX @inproceedings { DBLP:conf/trec/FoxS93 , author = {Edward A. Fox and Joseph A. Shaw} , title = {Combination of Multiple Searches} , booktitle = {{TREC}} , series = {{NIST} Special Publication} , volume = {500-215} , pages = {243--252} , publisher = {National Institute of Standards and Technology {(NIST)}} , year = {1993} }","title":"CombSUM"},{"location":"fusion/#condorcet","text":"Computes Condorcet as proposed by Montague et al. . BibTeX @inproceedings { DBLP:conf/cikm/MontagueA02 , author = {Mark H. Montague and Javed A. Aslam} , title = {Condorcet fusion for improved retrieval} , booktitle = {Proceedings of the 2002 {ACM} {CIKM} International Conference on Information and Knowledge Management, McLean, VA, USA, November 4-9, 2002} , pages = {538--548} , publisher = {{ACM}} , year = {2002} , url = {https://doi.org/10.1145/584792.584881} , doi = {10.1145/584792.584881} , timestamp = {Tue, 06 Nov 2018 16:57:50 +0100} , biburl = {https://dblp.org/rec/conf/cikm/MontagueA02.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"Condorcet"},{"location":"fusion/#isr","text":"Computes ISR as proposed by Mour\u00e3o et al. . BibTeX @article { DBLP:journals/cmig/MouraoMM15 , author = {Andr{\\'{e}} Mour{\\~{a}}o and Fl{\\'{a}}vio Martins and Jo{\\~{a}}o Magalh{\\~{a}}es} , title = {Multimodal medical information retrieval with unsupervised rank fusion} , journal = {Comput. Medical Imaging Graph.} , volume = {39} , pages = {35--45} , year = {2015} , url = {https://doi.org/10.1016/j.compmedimag.2014.05.006} , doi = {10.1016/j.compmedimag.2014.05.006} , timestamp = {Thu, 14 May 2020 10:17:16 +0200} , biburl = {https://dblp.org/rec/journals/cmig/MouraoMM15.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"ISR"},{"location":"fusion/#log_isr","text":"Computes Log_ISR as proposed by Mour\u00e3o et al. . BibTeX @article { DBLP:journals/cmig/MouraoMM15 , author = {Andr{\\'{e}} Mour{\\~{a}}o and Fl{\\'{a}}vio Martins and Jo{\\~{a}}o Magalh{\\~{a}}es} , title = {Multimodal medical information retrieval with unsupervised rank fusion} , journal = {Comput. Medical Imaging Graph.} , volume = {39} , pages = {35--45} , year = {2015} , url = {https://doi.org/10.1016/j.compmedimag.2014.05.006} , doi = {10.1016/j.compmedimag.2014.05.006} , timestamp = {Thu, 14 May 2020 10:17:16 +0200} , biburl = {https://dblp.org/rec/journals/cmig/MouraoMM15.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"Log_ISR"},{"location":"fusion/#logn_isr","text":"Computes Log_ISR as proposed by Mour\u00e3o et al. . BibTeX @article { DBLP:journals/cmig/MouraoMM15 , author = {Andr{\\'{e}} Mour{\\~{a}}o and Fl{\\'{a}}vio Martins and Jo{\\~{a}}o Magalh{\\~{a}}es} , title = {Multimodal medical information retrieval with unsupervised rank fusion} , journal = {Comput. Medical Imaging Graph.} , volume = {39} , pages = {35--45} , year = {2015} , url = {https://doi.org/10.1016/j.compmedimag.2014.05.006} , doi = {10.1016/j.compmedimag.2014.05.006} , timestamp = {Thu, 14 May 2020 10:17:16 +0200} , biburl = {https://dblp.org/rec/journals/cmig/MouraoMM15.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} } Optimization Parameter Default Value min_sigma 0.01 max_sigma 1.0 step 0.01","title":"LogN_ISR"},{"location":"fusion/#mapfuse","text":"Computes MAPFuse as proposed by Lillis et al. . BibTeX @inproceedings { DBLP:conf/sigir/LillisZTCLD10 , author = {David Lillis and Lusheng Zhang and Fergus Toolan and Rem W. Collier and David Leonard and John Dunnion} , editor = {Fabio Crestani and St{\\'{e}}phane Marchand{-}Maillet and Hsin{-}Hsi Chen and Efthimis N. Efthimiadis and Jacques Savoy} , title = {Estimating probabilities for effective data fusion} , booktitle = {Proceeding of the 33rd International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval, {SIGIR} 2010, Geneva, Switzerland, July 19-23, 2010} , pages = {347--354} , publisher = {{ACM}} , year = {2010} , url = {https://doi.org/10.1145/1835449.1835508} , doi = {10.1145/1835449.1835508} , timestamp = {Tue, 06 Nov 2018 11:07:25 +0100} , biburl = {https://dblp.org/rec/conf/sigir/LillisZTCLD10.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"MAPFuse"},{"location":"fusion/#mixed","text":"Computes Mixed as proposed by Wu et al. . BibTeX @inproceedings { DBLP:conf/cikm/WuC02 , author = {Shengli Wu and Fabio Crestani} , title = {Data fusion with estimated weights} , booktitle = {Proceedings of the 2002 {ACM} {CIKM} International Conference on Information and Knowledge Management, McLean, VA, USA, November 4-9, 2002} , pages = {648--651} , publisher = {{ACM}} , year = {2002} , url = {https://doi.org/10.1145/584792.584908} , doi = {10.1145/584792.584908} , timestamp = {Tue, 06 Nov 2018 16:57:40 +0100} , biburl = {https://dblp.org/rec/conf/cikm/WuC02.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} } Optimization Parameter Default Value step 0.1","title":"Mixed"},{"location":"fusion/#posfuse","text":"Computes PosFuse as proposed by Lillis et al. . BibTeX @inproceedings { DBLP:conf/sigir/LillisZTCLD10 , author = {David Lillis and Lusheng Zhang and Fergus Toolan and Rem W. Collier and David Leonard and John Dunnion} , editor = {Fabio Crestani and St{\\'{e}}phane Marchand{-}Maillet and Hsin{-}Hsi Chen and Efthimis N. Efthimiadis and Jacques Savoy} , title = {Estimating probabilities for effective data fusion} , booktitle = {Proceeding of the 33rd International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval, {SIGIR} 2010, Geneva, Switzerland, July 19-23, 2010} , pages = {347--354} , publisher = {{ACM}} , year = {2010} , url = {https://doi.org/10.1145/1835449.1835508} , doi = {10.1145/1835449.1835508} , timestamp = {Tue, 06 Nov 2018 11:07:25 +0100} , biburl = {https://dblp.org/rec/conf/sigir/LillisZTCLD10.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"PosFuse"},{"location":"fusion/#probfuse","text":"Computes ProbFuse as proposed by Lillis et al. . BibTeX @inproceedings { DBLP:conf/sigir/LillisTCD06 , author = {David Lillis and Fergus Toolan and Rem W. Collier and John Dunnion} , editor = {Efthimis N. Efthimiadis and Susan T. Dumais and David Hawking and Kalervo J{\\\"{a}}rvelin} , title = {ProbFuse: a probabilistic approach to data fusion} , booktitle = {{SIGIR} 2006: Proceedings of the 29th Annual International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval, Seattle, Washington, USA, August 6-11, 2006} , pages = {139--146} , publisher = {{ACM}} , year = {2006} , url = {https://doi.org/10.1145/1148170.1148197} , doi = {10.1145/1148170.1148197} , timestamp = {Wed, 14 Nov 2018 10:58:10 +0100} , biburl = {https://dblp.org/rec/conf/sigir/LillisTCD06.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} } Optimization Parameter Default Value min_n_segments 1 max_n_segments 100","title":"ProbFuse"},{"location":"fusion/#rank-biased-centroids-rbc","text":"Computes Rank-Biased Centroid (RBC) as proposed by Bailey et al. . BibTeX @inproceedings { DBLP:conf/sigir/BaileyMST17 , author = {Peter Bailey and Alistair Moffat and Falk Scholer and Paul Thomas} , editor = {Noriko Kando and Tetsuya Sakai and Hideo Joho and Hang Li and Arjen P. de Vries and Ryen W. White} , title = {Retrieval Consistency in the Presence of Query Variations} , booktitle = {Proceedings of the 40th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval, Shinjuku, Tokyo, Japan, August 7-11, 2017} , pages = {395--404} , publisher = {{ACM}} , year = {2017} , url = {https://doi.org/10.1145/3077136.3080839} , doi = {10.1145/3077136.3080839} , timestamp = {Wed, 25 Sep 2019 16:43:14 +0200} , biburl = {https://dblp.org/rec/conf/sigir/BaileyMST17.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} } Optimization Parameter Default Value min_phi 0.01 max_phi 1.0 step 0.01","title":"Rank-Biased Centroids (RBC)"},{"location":"fusion/#reciprocal-rank-fusion-rrf","text":"Computes Reciprocal Rank Fusion as proposed by Cormack et al. . BibTeX @inproceedings { DBLP:conf/sigir/CormackCB09 , author = {Gordon V. Cormack and Charles L. A. Clarke and Stefan B{\\\"{u}}ttcher} , title = {Reciprocal rank fusion outperforms condorcet and individual rank learning methods} , booktitle = {{SIGIR}} , pages = {758--759} , publisher = {{ACM}} , year = {2009} } Optimization Parameter Default Value min_k 10 max_k 100 step 10","title":"Reciprocal Rank Fusion (RRF)"},{"location":"fusion/#segfuse","text":"Computes SegFuse as proposed by Shokouhi . BibTeX @inproceedings { DBLP:conf/ecir/Shokouhi07a , author = {Milad Shokouhi} , editor = {Giambattista Amati and Claudio Carpineto and Giovanni Romano} , title = {Segmentation of Search Engine Results for Effective Data-Fusion} , booktitle = {Advances in Information Retrieval, 29th European Conference on {IR} Research, {ECIR} 2007, Rome, Italy, April 2-5, 2007, Proceedings} , series = {Lecture Notes in Computer Science} , volume = {4425} , pages = {185--197} , publisher = {Springer} , year = {2007} , url = {https://doi.org/10.1007/978-3-540-71496-5\\_19} , doi = {10.1007/978-3-540-71496-5\\_19} , timestamp = {Tue, 14 May 2019 10:00:37 +0200} , biburl = {https://dblp.org/rec/conf/ecir/Shokouhi07a.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} }","title":"SegFuse"},{"location":"fusion/#slidefuse","text":"Computes SlideFuse as proposed by Lillis et al. . BibTeX @inproceedings { DBLP:conf/ecir/LillisTCD08 , author = {David Lillis and Fergus Toolan and Rem W. Collier and John Dunnion} , editor = {Craig Macdonald and Iadh Ounis and Vassilis Plachouras and Ian Ruthven and Ryen W. White} , title = {Extending Probabilistic Data Fusion Using Sliding Windows} , booktitle = {Advances in Information Retrieval , 30th European Conference on {IR} Research, {ECIR} 2008, Glasgow, UK, March 30-April 3, 2008. Proceedings} , series = {Lecture Notes in Computer Science} , volume = {4956} , pages = {358--369} , publisher = {Springer} , year = {2008} , url = {https://doi.org/10.1007/978-3-540-78646-7\\_33} , doi = {10.1007/978-3-540-78646-7\\_33} , timestamp = {Sun, 25 Oct 2020 22:33:08 +0100} , biburl = {https://dblp.org/rec/conf/ecir/LillisTCD08.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} } Optimization Parameter Default Value min_w 1 max_w 100","title":"SlideFuse"},{"location":"fusion/#weighted-bordafuse","text":"Computes Weighted BordaFuse as proposed by Aslam et al. . BibTeX @inproceedings { DBLP:conf/sigir/AslamM01 , author = {Javed A. Aslam and Mark H. Montague} , editor = {W. Bruce Croft and David J. Harper and Donald H. Kraft and Justin Zobel} , title = {Models for Metasearch} , booktitle = {{SIGIR} 2001: Proceedings of the 24th Annual International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval, September 9-13, 2001, New Orleans, Louisiana, {USA}} , pages = {275--284} , publisher = {{ACM}} , year = {2001} , url = {https://doi.org/10.1145/383952.384007} , doi = {10.1145/383952.384007} , timestamp = {Tue, 06 Nov 2018 11:07:25 +0100} , biburl = {https://dblp.org/rec/conf/sigir/AslamM01.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} } Optimization Parameter Default Value step 0.1","title":"Weighted BordaFuse"},{"location":"fusion/#weighted-condorcet","text":"Computes Weighted Condorcet as proposed by Montague et al. . BibTeX @inproceedings { DBLP:conf/cikm/MontagueA02 , author = {Mark H. Montague and Javed A. Aslam} , title = {Condorcet fusion for improved retrieval} , booktitle = {Proceedings of the 2002 {ACM} {CIKM} International Conference on Information and Knowledge Management, McLean, VA, USA, November 4-9, 2002} , pages = {538--548} , publisher = {{ACM}} , year = {2002} , url = {https://doi.org/10.1145/584792.584881} , doi = {10.1145/584792.584881} , timestamp = {Tue, 06 Nov 2018 16:57:50 +0100} , biburl = {https://dblp.org/rec/conf/cikm/MontagueA02.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} } Optimization Parameter Default Value step 0.1","title":"Weighted Condorcet"},{"location":"fusion/#wmnz","text":"Computes Weighted MNZ as proposed by Wu et al. . BibTeX @inproceedings { DBLP:conf/cikm/WuC02 , author = {Shengli Wu and Fabio Crestani} , title = {Data fusion with estimated weights} , booktitle = {Proceedings of the 2002 {ACM} {CIKM} International Conference on Information and Knowledge Management, McLean, VA, USA, November 4-9, 2002} , pages = {648--651} , publisher = {{ACM}} , year = {2002} , url = {https://doi.org/10.1145/584792.584908} , doi = {10.1145/584792.584908} , timestamp = {Tue, 06 Nov 2018 16:57:40 +0100} , biburl = {https://dblp.org/rec/conf/cikm/WuC02.bib} , bibsource = {dblp computer science bibliography, https://dblp.org} } Optimization Parameter Default Value step 0.1","title":"WMNZ"},{"location":"fusion/#wighted-sum","text":"Computes a weighted sum of the scores given to documents by a list of Runs. Optimization Parameter Default Value step 0.1","title":"Wighted Sum"},{"location":"metrics/","text":"Metrics Aliases Aliases to use with ranx.evaluate and ranx.compare . Metric Alias @k .p Hits hits Yes No Hit Rate / Success hit_rate Yes No Precision precision Yes No Recall recall Yes No F1 f1 Yes No R-Precision r_precision No No Bpref bpref No No Rank-biased Precision rbp No Yes Mean Reciprocal Rank mrr Yes No Mean Average Precision map Yes No NDCG ndcg Yes No NDCG Burges ndcg_burges Yes No Hits Hits is the number of relevant documents retrieved. Hit Rate / Success Hit Rate is the fraction of queries for which at least one relevant document is retrieved. Note: it is equivalent to success from trec_eval . Precision Precision is the proportion of the retrieved documents that are relevant. \\[ \\operatorname{Precision}=\\frac{r}{n} \\] where, \\(r\\) is the number of retrieved relevant documents; \\(n\\) is the number of retrieved documents. Recall Recall is the ratio between the retrieved documents that are relevant and the total number of relevant documents. \\[ \\operatorname{Recall}=\\frac{r}{R} \\] where, \\(r\\) is the number of retrieved relevant documents; \\(R\\) is the total number of relevant documents. F1 F1 is the harmonic mean of Precision and Recall . \\[ \\operatorname{F1} = 2 \\times \\frac{\\operatorname{Precision} \\times \\operatorname{Recall}}{\\operatorname{Precision} + \\operatorname{Recall}} \\] R-Precision For a given query \\(Q\\) , R-Precision is the precision at \\(R\\) , where \\(R\\) is the number of relevant documents for \\(Q\\) . In other words, if there are \\(r\\) relevant documents among the top- \\(R\\) retrieved documents, then R-precision is: \\[ \\operatorname{R-Precision} = \\frac{r}{R} \\] Bpref Bpref is designed for situations where relevance judgments are known to be incomplete. It is defined as: \\[ \\operatorname{bpref}=\\frac{1}{R}\\sum_r{1 - \\frac{|\\text{$n$ ranked higher than $r$}|}{R}} \\] where, \\(r\\) is a relevant document; \\(n\\) is a member of the first R judged nonrelevant documents as retrieved by the system; \\(R\\) is the number of relevant documents. Rank-biased Precision Compute Rank-biased Precision (RBP). It is defined as: \\[ \\operatorname{RBP} = (1 - p) \\cdot \\sum_{i=1}^{d}{r_i \\cdot p^{i - 1}} \\] where, \\(p\\) is the persistence value; \\(r_i\\) is either 0 or 1, whether the \\(i\\) -th ranked document is non-relevant or relevant, repsectively. (Mean) Reciprocal Rank Reciprocal Rank is the multiplicative inverse of the rank of the first retrieved relevant document: 1 for first place, 1/2 for second place, 1/3 for third place, and so on. When averaged over many queries, it is usually called Mean Reciprocal Rank (MRR). \\[ Reciprocal Rank = \\frac{1}{rank} \\] where, \\(rank\\) is the position of the first retrieved relevant document. (Mean) Average Precision Average Precision is the average of the Precision scores computed after each relevant document is retrieved. When averaged over many queries, it is usually called Mean Average Precision (MAP). \\[ \\operatorname{Average Precision} = \\frac{\\sum_r \\operatorname{Precision}@r}{R} \\] where, \\(r\\) is the position of a relevant document; \\(R\\) is the total number of relevant documents. NDCG Compute Normalized Discounted Cumulative Gain (NDCG) as proposed by J\u00e4rvelin et al. . BibTeX @article { DBLP:journals/tois/JarvelinK02 , author = {Kalervo J{\\\"{a}}rvelin and Jaana Kek{\\\"{a}}l{\\\"{a}}inen} , title = {Cumulated gain-based evaluation of {IR} techniques} , journal = {{ACM} Trans. Inf. Syst.} , volume = {20} , number = {4} , pages = {422--446} , year = {2002} } \\[ \\operatorname{nDCG} = \\frac{\\operatorname{DCG}}{\\operatorname{IDCG}} \\] where, \\(\\operatorname{DCG}\\) is Discounted Cumulative Gain; \\(\\operatorname{IDCG}\\) is Ideal Discounted Cumulative Gain (max possibile DCG). NDCG Burges Compute Normalized Discounted Cumulative Gain (NDCG) at k as proposed by Burges et al. . BibTeX @inproceedings { DBLP:conf/icml/BurgesSRLDHH05 , author = {Christopher J. C. Burges and Tal Shaked and Erin Renshaw and Ari Lazier and Matt Deeds and Nicole Hamilton and Gregory N. Hullender} , title = {Learning to rank using gradient descent} , booktitle = {{ICML}} , series = {{ACM} International Conference Proceeding Series} , volume = {119} , pages = {89--96} , publisher = {{ACM}} , year = {2005} } \\[ \\operatorname{nDCG} = \\frac{\\operatorname{DCG}}{\\operatorname{IDCG}} \\] where, \\(\\operatorname{DCG}\\) is Discounted Cumulative Gain; \\(\\operatorname{IDCG}\\) is Ideal Discounted Cumulative Gain (max possibile DCG).","title":"Metrics"},{"location":"metrics/#metrics","text":"","title":"Metrics"},{"location":"metrics/#aliases","text":"Aliases to use with ranx.evaluate and ranx.compare . Metric Alias @k .p Hits hits Yes No Hit Rate / Success hit_rate Yes No Precision precision Yes No Recall recall Yes No F1 f1 Yes No R-Precision r_precision No No Bpref bpref No No Rank-biased Precision rbp No Yes Mean Reciprocal Rank mrr Yes No Mean Average Precision map Yes No NDCG ndcg Yes No NDCG Burges ndcg_burges Yes No","title":"Aliases"},{"location":"metrics/#hits","text":"Hits is the number of relevant documents retrieved.","title":"Hits"},{"location":"metrics/#hit-rate-success","text":"Hit Rate is the fraction of queries for which at least one relevant document is retrieved. Note: it is equivalent to success from trec_eval .","title":"Hit Rate / Success"},{"location":"metrics/#precision","text":"Precision is the proportion of the retrieved documents that are relevant. \\[ \\operatorname{Precision}=\\frac{r}{n} \\] where, \\(r\\) is the number of retrieved relevant documents; \\(n\\) is the number of retrieved documents.","title":"Precision"},{"location":"metrics/#recall","text":"Recall is the ratio between the retrieved documents that are relevant and the total number of relevant documents. \\[ \\operatorname{Recall}=\\frac{r}{R} \\] where, \\(r\\) is the number of retrieved relevant documents; \\(R\\) is the total number of relevant documents.","title":"Recall"},{"location":"metrics/#f1","text":"F1 is the harmonic mean of Precision and Recall . \\[ \\operatorname{F1} = 2 \\times \\frac{\\operatorname{Precision} \\times \\operatorname{Recall}}{\\operatorname{Precision} + \\operatorname{Recall}} \\]","title":"F1"},{"location":"metrics/#r-precision","text":"For a given query \\(Q\\) , R-Precision is the precision at \\(R\\) , where \\(R\\) is the number of relevant documents for \\(Q\\) . In other words, if there are \\(r\\) relevant documents among the top- \\(R\\) retrieved documents, then R-precision is: \\[ \\operatorname{R-Precision} = \\frac{r}{R} \\]","title":"R-Precision"},{"location":"metrics/#bpref","text":"Bpref is designed for situations where relevance judgments are known to be incomplete. It is defined as: \\[ \\operatorname{bpref}=\\frac{1}{R}\\sum_r{1 - \\frac{|\\text{$n$ ranked higher than $r$}|}{R}} \\] where, \\(r\\) is a relevant document; \\(n\\) is a member of the first R judged nonrelevant documents as retrieved by the system; \\(R\\) is the number of relevant documents.","title":"Bpref"},{"location":"metrics/#rank-biased-precision","text":"Compute Rank-biased Precision (RBP). It is defined as: \\[ \\operatorname{RBP} = (1 - p) \\cdot \\sum_{i=1}^{d}{r_i \\cdot p^{i - 1}} \\] where, \\(p\\) is the persistence value; \\(r_i\\) is either 0 or 1, whether the \\(i\\) -th ranked document is non-relevant or relevant, repsectively.","title":"Rank-biased Precision"},{"location":"metrics/#mean-reciprocal-rank","text":"Reciprocal Rank is the multiplicative inverse of the rank of the first retrieved relevant document: 1 for first place, 1/2 for second place, 1/3 for third place, and so on. When averaged over many queries, it is usually called Mean Reciprocal Rank (MRR). \\[ Reciprocal Rank = \\frac{1}{rank} \\] where, \\(rank\\) is the position of the first retrieved relevant document.","title":"(Mean) Reciprocal Rank"},{"location":"metrics/#mean-average-precision","text":"Average Precision is the average of the Precision scores computed after each relevant document is retrieved. When averaged over many queries, it is usually called Mean Average Precision (MAP). \\[ \\operatorname{Average Precision} = \\frac{\\sum_r \\operatorname{Precision}@r}{R} \\] where, \\(r\\) is the position of a relevant document; \\(R\\) is the total number of relevant documents.","title":"(Mean) Average Precision"},{"location":"metrics/#ndcg","text":"Compute Normalized Discounted Cumulative Gain (NDCG) as proposed by J\u00e4rvelin et al. . BibTeX @article { DBLP:journals/tois/JarvelinK02 , author = {Kalervo J{\\\"{a}}rvelin and Jaana Kek{\\\"{a}}l{\\\"{a}}inen} , title = {Cumulated gain-based evaluation of {IR} techniques} , journal = {{ACM} Trans. Inf. Syst.} , volume = {20} , number = {4} , pages = {422--446} , year = {2002} } \\[ \\operatorname{nDCG} = \\frac{\\operatorname{DCG}}{\\operatorname{IDCG}} \\] where, \\(\\operatorname{DCG}\\) is Discounted Cumulative Gain; \\(\\operatorname{IDCG}\\) is Ideal Discounted Cumulative Gain (max possibile DCG).","title":"NDCG"},{"location":"metrics/#ndcg-burges","text":"Compute Normalized Discounted Cumulative Gain (NDCG) at k as proposed by Burges et al. . BibTeX @inproceedings { DBLP:conf/icml/BurgesSRLDHH05 , author = {Christopher J. C. Burges and Tal Shaked and Erin Renshaw and Ari Lazier and Matt Deeds and Nicole Hamilton and Gregory N. Hullender} , title = {Learning to rank using gradient descent} , booktitle = {{ICML}} , series = {{ACM} International Conference Proceeding Series} , volume = {119} , pages = {89--96} , publisher = {{ACM}} , year = {2005} } \\[ \\operatorname{nDCG} = \\frac{\\operatorname{DCG}}{\\operatorname{IDCG}} \\] where, \\(\\operatorname{DCG}\\) is Discounted Cumulative Gain; \\(\\operatorname{IDCG}\\) is Ideal Discounted Cumulative Gain (max possibile DCG).","title":"NDCG Burges"},{"location":"normalization/","text":"Normalization ranx provides several result lists normalization strategies to be used conjunctly with the fusion methods. Normalization aims at transforming the scores of a result list into new values to make them comparable with those of other normalized result lists, which is mandatory for correctly applying many of the provided fusion methods. The normalization strategy to apply before fusion can be defined through the norm parameter of the functions fuse and optimize_fusion (defaults to min-max ). Normalization Strategies Alias Min-Max Norm min-max Max Norm max Sum Norm sum ZMUV Norm zmuv Rank Norm rank Borda Norm borda Min-Max Norm Min-Max Norm scales the scores (s) of a result list between 0 and 1, scaling to 0 the minimum score ( \\(s_{min}\\) ) and 1 the maximum score ( \\(s_{max}\\) ). \\[ \\operatorname{MinMaxNorm(s)}=\\frac{s - s_{min}}{s_{max} - s_{min}} \\] Max Norm Max Norm scales the scores (s) of a result list the maximum score ( \\(s_{max}\\) ) is scaled to 1. \\[ \\operatorname{MaxNorm(s)}=\\frac{s}{s_{max}} \\] Sum Norm Sum Norm scales the minimum score ( \\(s_{min}\\) ) to 0 and the scores sum to 1. It is computed as follows: \\[ \\operatorname{SumNorm(s)}=\\frac{s - s_{min}}{\\sum_s{s - s_{min}}} \\] ZMUV Norm ZMUV Norm (zero-mean, unit-variance) scales the scores so that their mean ( \\(s_{mean}\\) ) becomes zero and their variance 1. \\[ \\operatorname{ZMUVNorm(s)}=\\frac{s - s_{mean}}{s_{std}} \\] Rank Norm Rank Norm transforms the scores according to the position in the ranking of the results they are associated with. In this case, the normalized scores are uniformly distributed. The top-ranked result gets a score of 1, while the bottom-ranked result gets a score of \\(\\frac{1}{|r|}\\) , where \\(|r|\\) is the number of results in the ranked list. \\[ \\operatorname{RankNorm(s_i)}=1-\\frac{r_i - 1}{|r|} \\] Borda Norm Borda Norm transforms the scores in a similar manner of how BordaFuse assign points to the results before fusing multiple runs. Borda Norm is defined as follows: \\[ \\operatorname{BordaNorm(s_i)}= \\begin{cases} 1 - \\frac{r_i - 1}{|candidates|} & \\mathit{if}\\ d \\in r \\\\ \\frac{1}{2} - \\frac{|r|-1}{2 \\cdot |candidates|} & \\mathit{otherwise} \\end{cases} \\] Please, refer to Renda et al. for further details.","title":"Normalization"},{"location":"normalization/#normalization","text":"ranx provides several result lists normalization strategies to be used conjunctly with the fusion methods. Normalization aims at transforming the scores of a result list into new values to make them comparable with those of other normalized result lists, which is mandatory for correctly applying many of the provided fusion methods. The normalization strategy to apply before fusion can be defined through the norm parameter of the functions fuse and optimize_fusion (defaults to min-max ). Normalization Strategies Alias Min-Max Norm min-max Max Norm max Sum Norm sum ZMUV Norm zmuv Rank Norm rank Borda Norm borda","title":"Normalization"},{"location":"normalization/#min-max-norm","text":"Min-Max Norm scales the scores (s) of a result list between 0 and 1, scaling to 0 the minimum score ( \\(s_{min}\\) ) and 1 the maximum score ( \\(s_{max}\\) ). \\[ \\operatorname{MinMaxNorm(s)}=\\frac{s - s_{min}}{s_{max} - s_{min}} \\]","title":"Min-Max Norm"},{"location":"normalization/#max-norm","text":"Max Norm scales the scores (s) of a result list the maximum score ( \\(s_{max}\\) ) is scaled to 1. \\[ \\operatorname{MaxNorm(s)}=\\frac{s}{s_{max}} \\]","title":"Max Norm"},{"location":"normalization/#sum-norm","text":"Sum Norm scales the minimum score ( \\(s_{min}\\) ) to 0 and the scores sum to 1. It is computed as follows: \\[ \\operatorname{SumNorm(s)}=\\frac{s - s_{min}}{\\sum_s{s - s_{min}}} \\]","title":"Sum Norm"},{"location":"normalization/#zmuv-norm","text":"ZMUV Norm (zero-mean, unit-variance) scales the scores so that their mean ( \\(s_{mean}\\) ) becomes zero and their variance 1. \\[ \\operatorname{ZMUVNorm(s)}=\\frac{s - s_{mean}}{s_{std}} \\]","title":"ZMUV Norm"},{"location":"normalization/#rank-norm","text":"Rank Norm transforms the scores according to the position in the ranking of the results they are associated with. In this case, the normalized scores are uniformly distributed. The top-ranked result gets a score of 1, while the bottom-ranked result gets a score of \\(\\frac{1}{|r|}\\) , where \\(|r|\\) is the number of results in the ranked list. \\[ \\operatorname{RankNorm(s_i)}=1-\\frac{r_i - 1}{|r|} \\]","title":"Rank Norm"},{"location":"normalization/#borda-norm","text":"Borda Norm transforms the scores in a similar manner of how BordaFuse assign points to the results before fusing multiple runs. Borda Norm is defined as follows: \\[ \\operatorname{BordaNorm(s_i)}= \\begin{cases} 1 - \\frac{r_i - 1}{|candidates|} & \\mathit{if}\\ d \\in r \\\\ \\frac{1}{2} - \\frac{|r|-1}{2 \\cdot |candidates|} & \\mathit{otherwise} \\end{cases} \\] Please, refer to Renda et al. for further details.","title":"Borda Norm"},{"location":"qrels/","text":"Qrels Qrels , or query relevance judgments , stores the ground truth for conducting evaluations. The preferred way for creating a Qrels istance is converting Python dictionary as follows: from ranx import Qrels qrels_dict = { \"q_1\" : { \"d_1\" : 1 , \"d_2\" : 2 , }, \"q_2\" : { \"d_3\" : 2 , \"d_2\" : 1 , \"d_5\" : 3 , }, } qrels = Qrels ( qrels_dict , name = \"MSMARCO\" ) Qrels can also be loaded from TREC-style and JSON files, from ir-datasets , and from Pandas DataFrames. Load from files Parse a qrels file into ranx.Qrels . Supported formats are JSON and TREC qrels format. Correct import behavior is inferred from the file extension: .json \u2192 json , .trec \u2192 trec , .txt \u2192 trec . Use the kind argument to override the default behavior. qrels = Qrels . from_file ( \"path/to/qrels.json\" ) # JSON file qrels = Qrels . from_file ( \"path/to/qrels.trec\" ) # TREC-Style file qrels = Qrels . from_file ( \"path/to/qrels.txt\" ) # TREC-Style file with txt extension qrels = Qrels . from_file ( \"path/to/qrels.custom\" , kind = \"json\" ) # Loaded as JSON file Load from ir-datasets You can find the full list of the qrels provided by ir-datasets here . qrels = Qrels . from_ir_datasets ( \"msmarco-document/dev\" ) Load from Pandas DataFrames from pandas import DataFrame qrels_df = DataFrame . from_dict ({ \"q_id\" : [ \"q_1\" , \"q_1\" , \"q_2\" , \"q_2\" ], \"doc_id\" : [ \"d_12\" , \"d_25\" , \"d_11\" , \"d_22\" ], \"score\" : [ 5 , 3 , 6 , 1 ], }) qrels = Qrels . from_df ( df = qrels_df , q_id_col = \"q_id\" , doc_id_col = \"doc_id\" , score_col = \"score\" , ) Save Write qrels to path as JSON file or TREC qrels format. File type is automatically inferred form the filename extension: .json \u2192 json , .trec \u2192 trec , .txt \u2192 trec . Use the kind argument to override the default behavior. qrels . save ( \"path/to/qrels.json\" ) # Save as JSON file qrels . save ( \"path/to/qrels.trec\" ) # Save as TREC-Style file qrels . save ( \"path/to/qrels.txt\" ) # Save as TREC-Style file with txt extension qrels . save ( \"path/to/qrels.custom\" , kind = \"json\" ) # Save as JSON file","title":"Qrels"},{"location":"qrels/#qrels","text":"Qrels , or query relevance judgments , stores the ground truth for conducting evaluations. The preferred way for creating a Qrels istance is converting Python dictionary as follows: from ranx import Qrels qrels_dict = { \"q_1\" : { \"d_1\" : 1 , \"d_2\" : 2 , }, \"q_2\" : { \"d_3\" : 2 , \"d_2\" : 1 , \"d_5\" : 3 , }, } qrels = Qrels ( qrels_dict , name = \"MSMARCO\" ) Qrels can also be loaded from TREC-style and JSON files, from ir-datasets , and from Pandas DataFrames.","title":"Qrels"},{"location":"qrels/#load-from-files","text":"Parse a qrels file into ranx.Qrels . Supported formats are JSON and TREC qrels format. Correct import behavior is inferred from the file extension: .json \u2192 json , .trec \u2192 trec , .txt \u2192 trec . Use the kind argument to override the default behavior. qrels = Qrels . from_file ( \"path/to/qrels.json\" ) # JSON file qrels = Qrels . from_file ( \"path/to/qrels.trec\" ) # TREC-Style file qrels = Qrels . from_file ( \"path/to/qrels.txt\" ) # TREC-Style file with txt extension qrels = Qrels . from_file ( \"path/to/qrels.custom\" , kind = \"json\" ) # Loaded as JSON file","title":"Load from files"},{"location":"qrels/#load-from-ir-datasets","text":"You can find the full list of the qrels provided by ir-datasets here . qrels = Qrels . from_ir_datasets ( \"msmarco-document/dev\" )","title":"Load from ir-datasets"},{"location":"qrels/#load-from-pandas-dataframes","text":"from pandas import DataFrame qrels_df = DataFrame . from_dict ({ \"q_id\" : [ \"q_1\" , \"q_1\" , \"q_2\" , \"q_2\" ], \"doc_id\" : [ \"d_12\" , \"d_25\" , \"d_11\" , \"d_22\" ], \"score\" : [ 5 , 3 , 6 , 1 ], }) qrels = Qrels . from_df ( df = qrels_df , q_id_col = \"q_id\" , doc_id_col = \"doc_id\" , score_col = \"score\" , )","title":"Load from Pandas DataFrames"},{"location":"qrels/#save","text":"Write qrels to path as JSON file or TREC qrels format. File type is automatically inferred form the filename extension: .json \u2192 json , .trec \u2192 trec , .txt \u2192 trec . Use the kind argument to override the default behavior. qrels . save ( \"path/to/qrels.json\" ) # Save as JSON file qrels . save ( \"path/to/qrels.trec\" ) # Save as TREC-Style file qrels . save ( \"path/to/qrels.txt\" ) # Save as TREC-Style file with txt extension qrels . save ( \"path/to/qrels.custom\" , kind = \"json\" ) # Save as JSON file","title":"Save"},{"location":"report/","text":"Report A Report instance is automatically generated as the results of a comparison. A Report provides a convenient way of inspecting a comparison results and exporting those il LaTeX for your scientific publications. By changing the values of the parameters rounding_digits (int) and show_percentages (bool) you can control what is shown on printing and when generating LaTeX tables. from ranx import compare # Compare different runs and perform statistical tests report = compare ( qrels = qrels , runs = [ run_1 , run_2 , run_3 , run_4 , run_5 ], metrics = [ \"map@100\" , \"mrr@100\" , \"ndcg@10\" ], max_p = 0.01 # P-value threshold ) print ( report ) Output: # Model MAP@100 MRR@100 NDCG@10 --- ------- ---------- ---------- ---------- a model_1 0.3202\u1d47 0.3207\u1d47 0.3684\u1d47\u1d9c b model_2 0.2332 0.2339 0.239 c model_3 0.3082\u1d47 0.3089\u1d47 0.3295\u1d47 d model_4 0.3664\u1d43\u1d47\u1d9c 0.3668\u1d43\u1d47\u1d9c 0.4078\u1d43\u1d47\u1d9c e model_5 0.4053\u1d43\u1d47\u1d9c\u1d48 0.4061\u1d43\u1d47\u1d9c\u1d48 0.4512\u1d43\u1d47\u1d9c\u1d48 print ( report . to_latex ()) # To get the LaTeX code","title":"Report"},{"location":"report/#report","text":"A Report instance is automatically generated as the results of a comparison. A Report provides a convenient way of inspecting a comparison results and exporting those il LaTeX for your scientific publications. By changing the values of the parameters rounding_digits (int) and show_percentages (bool) you can control what is shown on printing and when generating LaTeX tables. from ranx import compare # Compare different runs and perform statistical tests report = compare ( qrels = qrels , runs = [ run_1 , run_2 , run_3 , run_4 , run_5 ], metrics = [ \"map@100\" , \"mrr@100\" , \"ndcg@10\" ], max_p = 0.01 # P-value threshold ) print ( report ) Output: # Model MAP@100 MRR@100 NDCG@10 --- ------- ---------- ---------- ---------- a model_1 0.3202\u1d47 0.3207\u1d47 0.3684\u1d47\u1d9c b model_2 0.2332 0.2339 0.239 c model_3 0.3082\u1d47 0.3089\u1d47 0.3295\u1d47 d model_4 0.3664\u1d43\u1d47\u1d9c 0.3668\u1d43\u1d47\u1d9c 0.4078\u1d43\u1d47\u1d9c e model_5 0.4053\u1d43\u1d47\u1d9c\u1d48 0.4061\u1d43\u1d47\u1d9c\u1d48 0.4512\u1d43\u1d47\u1d9c\u1d48 print ( report . to_latex ()) # To get the LaTeX code","title":"Report"},{"location":"run/","text":"Run Run stores the relevance scores estimated by the model under evaluation. The preferred way for creating a Run istance is converting a Python dictionary as follows: from ranx import Run run_dict = { \"q_1\" : { \"d_1\" : 1.5 , \"d_2\" : 2.6 , }, \"q_2\" : { \"d_3\" : 2.8 , \"d_2\" : 1.2 , \"d_5\" : 3.1 , }, } run = Run ( run_dict , name = \"bm25\" ) Runs can also be loaded from TREC-style and JSON files, and from Pandas DataFrames. Load from Files Parse a run file into ranx.Run . Supported formats are JSON and TREC run format. Correct import behavior is inferred from the file extension: .json \u2192 json , .trec \u2192 trec , .txt \u2192 trec . Use the kind argument to override the default behavior. run = Run . from_file ( \"path/to/run.json\" ) # JSON file run = Run . from_file ( \"path/to/run.trec\" ) # TREC-Style file run = Run . from_file ( \"path/to/run.txt\" ) # TREC-Style file with txt extension run = Run . from_file ( \"path/to/run.custom\" , kind = \"json\" ) # Loaded as JSON file Load from Pandas DataFrames from pandas import DataFrame run_df = DataFrame . from_dict ({ \"q_id\" : [ \"q_1\" , \"q_1\" , \"q_2\" , \"q_2\" ], \"doc_id\" : [ \"d_12\" , \"d_25\" , \"d_11\" , \"d_22\" ], \"score\" : [ 0.5 , 0.3 , 0.6 , 0.1 ], }) run = Runs . from_df ( df = run_df , q_id_col = \"q_id\" , doc_id_col = \"doc_id\" , score_col = \"score\" , ) Save Write run to path as JSON file or TREC run format. File type is automatically inferred form the filename extension: .json \u2192 json , .trec \u2192 trec , .txt \u2192 trec . Use the kind argument to override the default behavior. run . save ( \"path/to/run.json\" ) # Save as JSON file run . save ( \"path/to/run.trec\" ) # Save as TREC-Style file run . save ( \"path/to/run.txt\" ) # Save as TREC-Style file with txt extension run . save ( \"path/to/run.custom\" , kind = \"json\" ) # Save as JSON file","title":"Run"},{"location":"run/#run","text":"Run stores the relevance scores estimated by the model under evaluation. The preferred way for creating a Run istance is converting a Python dictionary as follows: from ranx import Run run_dict = { \"q_1\" : { \"d_1\" : 1.5 , \"d_2\" : 2.6 , }, \"q_2\" : { \"d_3\" : 2.8 , \"d_2\" : 1.2 , \"d_5\" : 3.1 , }, } run = Run ( run_dict , name = \"bm25\" ) Runs can also be loaded from TREC-style and JSON files, and from Pandas DataFrames.","title":"Run"},{"location":"run/#load-from-files","text":"Parse a run file into ranx.Run . Supported formats are JSON and TREC run format. Correct import behavior is inferred from the file extension: .json \u2192 json , .trec \u2192 trec , .txt \u2192 trec . Use the kind argument to override the default behavior. run = Run . from_file ( \"path/to/run.json\" ) # JSON file run = Run . from_file ( \"path/to/run.trec\" ) # TREC-Style file run = Run . from_file ( \"path/to/run.txt\" ) # TREC-Style file with txt extension run = Run . from_file ( \"path/to/run.custom\" , kind = \"json\" ) # Loaded as JSON file","title":"Load from Files"},{"location":"run/#load-from-pandas-dataframes","text":"from pandas import DataFrame run_df = DataFrame . from_dict ({ \"q_id\" : [ \"q_1\" , \"q_1\" , \"q_2\" , \"q_2\" ], \"doc_id\" : [ \"d_12\" , \"d_25\" , \"d_11\" , \"d_22\" ], \"score\" : [ 0.5 , 0.3 , 0.6 , 0.1 ], }) run = Runs . from_df ( df = run_df , q_id_col = \"q_id\" , doc_id_col = \"doc_id\" , score_col = \"score\" , )","title":"Load from Pandas DataFrames"},{"location":"run/#save","text":"Write run to path as JSON file or TREC run format. File type is automatically inferred form the filename extension: .json \u2192 json , .trec \u2192 trec , .txt \u2192 trec . Use the kind argument to override the default behavior. run . save ( \"path/to/run.json\" ) # Save as JSON file run . save ( \"path/to/run.trec\" ) # Save as TREC-Style file run . save ( \"path/to/run.txt\" ) # Save as TREC-Style file with txt extension run . save ( \"path/to/run.custom\" , kind = \"json\" ) # Save as JSON file","title":"Save"},{"location":"stat_tests/","text":"Statistical Tests ranx provides two statistical tests that can be used when comparing different runs: Fisher's Randomization Test Two-sided Paired Student's t-Test Please, refer to Smucker et al. for additional information on statistical tests for Information Retrieval. To use the Fisher's Randomization Test , pass stat_test=\"fisher\" to compare . To use the Two-sided Paired Student's t-Test , pass stat_test=\"student\" to compare .","title":"Statistical Tests"},{"location":"stat_tests/#statistical-tests","text":"ranx provides two statistical tests that can be used when comparing different runs: Fisher's Randomization Test Two-sided Paired Student's t-Test Please, refer to Smucker et al. for additional information on statistical tests for Information Retrieval. To use the Fisher's Randomization Test , pass stat_test=\"fisher\" to compare . To use the Two-sided Paired Student's t-Test , pass stat_test=\"student\" to compare .","title":"Statistical Tests"}]}